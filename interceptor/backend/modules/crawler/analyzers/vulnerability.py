# galdr/interceptor/backend/modules/crawler/analyzers/vulnerability.py
import re
import json
import aiohttp
from typing import List, Dict, Optional
from datetime import datetime

from ..models.crawl_data import CrawlEntry


class VulnerabilityAnalyzer:
    """Analyzes content for potential vulnerabilities"""
    
    def __init__(self, config):
        self.config = config
        self.cve_database = self._load_cve_database()
        self.vulnerability_patterns = self._initialize_patterns()
        self.ai_analyzer = None
        
        if config.enable_ai_analysis and config.llm_api_key:
            from .ai_analyzer import AIVulnAnalyzer
            self.ai_analyzer = AIVulnAnalyzer(config)
    
    def _load_cve_database(self) -> Dict:
        """Load CVE database for vulnerability matching"""
        try:
            with open(self.config.cve_database_path, 'r') as f:
                return json.load(f)
        except Exception:
            # Return basic vulnerability patterns if CVE database not available
            return {
                "patterns": {
                    "SQL_INJECTION": [
                        r"error.*sql",
                        r"mysql_fetch_array",
                        r"ORA-\d+",
                        r"Microsoft.*ODBC.*SQL Server"
                    ],
                    "XSS": [
                        r"<script[^>]*>.*?</script>",
                        r"javascript:",
                        r"onerror\s*=",
                        r"onload\s*="
                    ],
                    "PATH_TRAVERSAL": [
                        r"\.\./",
                        r"\.\.\\",
                        r"file://",
                        r"/etc/passwd"
                    ],
                    "INFO_DISCLOSURE": [
                        r"debug.*true",
                        r"stack trace",
                        r"phpinfo\(\)",
                        r"server error"
                    ]
                }
            }
    
    def _initialize_patterns(self) -> Dict:
        """Initialize vulnerability detection patterns"""
        return {
            # Common vulnerability indicators
            "error_disclosure": [
                r"(?i)(fatal error|warning|notice).*line \d+",
                r"(?i)stack trace",
                r"(?i)exception.*at line",
                r"(?i)error.*mysql",
                r"(?i)ora-\d{5}",
                r"(?i)microsoft.*odbc.*error"
            ],
            
            "debug_information": [
                r"(?i)debug\s*[:=]\s*true",
                r"(?i)development\s*mode",
                r"(?i)phpinfo\(\)",
                r"(?i)var_dump\(",
                r"(?i)print_r\(",
                r"(?i)console\.log\("
            ],
            
            "sensitive_paths": [
                r"/admin",
                r"/administrator",
                r"/wp-admin",
                r"/phpmyadmin",
                r"/cpanel",
                r"/.env",
                r"/config",
                r"/backup"
            ],
            
            "version_disclosure": [
                r"(?i)powered by.*\d+\.\d+",
                r"(?i)version.*\d+\.\d+\.\d+",
                r"(?i)generator.*\d+\.\d+"
            ],
            
            "weak_crypto": [
                r"(?i)md5\(",
                r"(?i)sha1\(",
                r"(?i)des[_-]encrypt",
                r"(?i)rc4"
            ],
            
            "injection_vectors": [
                r"(?i)eval\s*\(",
                r"(?i)exec\s*\(",
                r"(?i)system\s*\(",
                r"(?i)shell_exec\s*\(",
                r"(?i)passthru\s*\(",
                r"(?i)file_get_contents\s*\(\s*\$"
            ]
        }
    
    async def analyze(self, entry: CrawlEntry) -> List[Dict]:
        """Analyze entry for vulnerabilities"""
        vulnerabilities = []
        
        try:
            # Pattern-based detection
            pattern_vulns = self._analyze_patterns(entry)
            vulnerabilities.extend(pattern_vulns)
            
            # Header-based detection
            header_vulns = self._analyze_headers(entry)
            vulnerabilities.extend(header_vulns)
            
            # CVE database matching
            cve_vulns = self._analyze_cve_patterns(entry)
            vulnerabilities.extend(cve_vulns)
            
            # AI-enhanced detection (if enabled)
            if self.ai_analyzer:
                ai_vulns = await self.ai_analyzer.analyze(entry)
                vulnerabilities.extend(ai_vulns)
            
            # Filter by confidence threshold
            vulnerabilities = [
                vuln for vuln in vulnerabilities 
                if vuln.get('confidence', 0) >= self.config.vulnerability_confidence_threshold
            ]
            
        except Exception as e:
            self.logger.error(f"Error analyzing vulnerabilities: {e}")
        
        return vulnerabilities
    
    def _analyze_patterns(self, entry: CrawlEntry) -> List[Dict]:
        """Analyze using vulnerability patterns"""
        vulnerabilities = []
        
        for vuln_type, patterns in self.vulnerability_patterns.items():
            for pattern in patterns:
                matches = re.finditer(pattern, entry.response_body, re.IGNORECASE | re.MULTILINE)
                
                for match in matches:
                    vulnerability = {
                        'type': vuln_type.replace('_', ' ').title(),
                        'severity': self._determine_severity(vuln_type),
                        'confidence': 0.8,
                        'description': self._get_vulnerability_description(vuln_type),
                        'evidence': match.group(0)[:200],
                        'location': {
                            'url': entry.url,
                            'position': match.start()
                        },
                        'timestamp': datetime.now().isoformat(),
                        'remediation': self._get_remediation_advice(vuln_type)
                    }
                    vulnerabilities.append(vulnerability)
        
        return vulnerabilities
    
    def _analyze_headers(self, entry: CrawlEntry) -> List[Dict]:
        """Analyze HTTP headers for security issues"""
        vulnerabilities = []
        
        # Missing security headers
        security_headers = {
            'X-Frame-Options': 'Clickjacking protection missing',
            'X-XSS-Protection': 'XSS protection header missing',
            'X-Content-Type-Options': 'Content type sniffing protection missing',
            'Strict-Transport-Security': 'HSTS header missing',
            'Content-Security-Policy': 'CSP header missing'
        }
        
        for header, description in security_headers.items():
            if header not in entry.response_headers:
                vulnerability = {
                    'type': 'Missing Security Header',
                    'severity': 'Medium',
                    'confidence': 0.9,
                    'description': description,
                    'evidence': f"Missing {header} header",
                    'location': {
                        'url': entry.url,
                        'type': 'header'
                    },
                    'timestamp': datetime.now().isoformat(),
                    'remediation': f'Add {header} header with appropriate value'
                }
                vulnerabilities.append(vulnerability)
        
        # Insecure header values
        server_header = entry.response_headers.get('Server', '')
        if server_header and re.search(r'\d+\.\d+', server_header):
            vulnerability = {
                'type': 'Information Disclosure',
                'severity': 'Low',
                'confidence': 0.8,
                'description': 'Server version disclosed in headers',
                'evidence': f"Server: {server_header}",
                'location': {
                    'url': entry.url,
                    'type': 'header'
                },
                'timestamp': datetime.now().isoformat(),
                'remediation': 'Configure server to hide version information'
            }
            vulnerabilities.append(vulnerability)
        
        return vulnerabilities
    
    def _analyze_cve_patterns(self, entry: CrawlEntry) -> List[Dict]:
        """Analyze against CVE database patterns"""
        vulnerabilities = []
        
        if 'patterns' in self.cve_database:
            for cve_type, patterns in self.cve_database['patterns'].items():
                for pattern in patterns:
                    if re.search(pattern, entry.response_body, re.IGNORECASE):
                        vulnerability = {
                            'type': cve_type.replace('_', ' ').title(),
                            'severity': 'High',
                            'confidence': 0.9,
                            'description': f'Potential {cve_type} vulnerability detected',
                            'evidence': pattern,
                            'location': {
                                'url': entry.url,
                                'pattern': pattern
                            },
                            'timestamp': datetime.now().isoformat(),
                            'remediation': 'Review code for vulnerability and apply appropriate fixes'
                        }
                        vulnerabilities.append(vulnerability)
        
        return vulnerabilities
    
    def _determine_severity(self, vuln_type: str) -> str:
        """Determine vulnerability severity"""
        severity_map = {
            'injection_vectors': 'Critical',
            'error_disclosure': 'Medium',
            'debug_information': 'Medium',
            'sensitive_paths': 'High',
            'version_disclosure': 'Low',
            'weak_crypto': 'High'
        }
        return severity_map.get(vuln_type, 'Medium')
    
    def _get_vulnerability_description(self, vuln_type: str) -> str:
        """Get description for vulnerability type"""
        descriptions = {
            'error_disclosure': 'Application errors are being disclosed to users, potentially revealing sensitive information about the system architecture.',
            'debug_information': 'Debug information is exposed in the response, which could aid attackers in understanding the application structure.',
            'sensitive_paths': 'Sensitive administrative paths or files are accessible or referenced.',
            'version_disclosure': 'Software version information is disclosed, which could help attackers identify known vulnerabilities.',
            'weak_crypto': 'Weak cryptographic functions are being used, which may compromise data security.',
            'injection_vectors': 'Potential code injection vulnerabilities detected in the application.'
        }
        return descriptions.get(vuln_type, 'Potential security vulnerability detected.')
    
    def _get_remediation_advice(self, vuln_type: str) -> str:
        """Get remediation advice for vulnerability type"""
        remediation = {
            'error_disclosure': 'Implement proper error handling and avoid displaying detailed error messages to users.',
            'debug_information': 'Disable debug mode in production environments and remove debug output.',
            'sensitive_paths': 'Restrict access to administrative areas and sensitive files using proper authentication and authorization.',
            'version_disclosure': 'Configure web server and applications to hide version information.',
            'weak_crypto': 'Replace weak cryptographic functions with secure alternatives (e.g., use bcrypt instead of MD5).',
            'injection_vectors': 'Implement input validation, parameterized queries, and avoid using dangerous functions with user input.'
        }
        return remediation.get(vuln_type, 'Review the finding and implement appropriate security measures.')


class AIVulnAnalyzer:
    """AI-enhanced vulnerability analyzer"""
    
    def __init__(self, config):
        self.config = config
        self.client = self._initialize_ai_client()
    
    def _initialize_ai_client(self):
        """Initialize AI client"""
        if self.config.llm_provider == "openai":
            import openai
            return openai.AsyncOpenAI(api_key=self.config.llm_api_key)
        elif self.config.llm_provider == "anthropic":
            import anthropic
            return anthropic.AsyncAnthropic(api_key=self.config.llm_api_key)
        return None
    
    async def analyze(self, entry: CrawlEntry) -> List[Dict]:
        """Use AI to detect potential vulnerabilities"""
        if not self.client:
            return []
        
        try:
            # Prepare content for analysis
            analysis_content = self._prepare_content(entry)
            
            # Generate AI prompt
            prompt = self._create_vulnerability_prompt(analysis_content, entry.url)
            
            # Get AI response
            response = await self._get_ai_response(prompt)
            
            # Parse vulnerabilities from response
            vulnerabilities = self._parse_vulnerabilities(response, entry.url)
            
            return vulnerabilities
            
        except Exception as e:
            self.logger.error(f"AI vulnerability analysis error: {e}")
            return []
    
    def _prepare_content(self, entry: CrawlEntry) -> str:
        """Prepare content for AI analysis"""
        max_content_length = 3000
        
        content_parts = []
        
        # Add relevant headers
        security_headers = ['Server', 'X-Powered-By', 'Set-Cookie', 'Location']
        content_parts.append("=== RESPONSE HEADERS ===")
        for header in security_headers:
            if header in entry.response_headers:
                content_parts.append(f"{header}: {entry.response_headers[header]}")
        
        # Add status code
        content_parts.append(f"\nStatus Code: {entry.status_code}")
        
        # Add relevant body content
        content_parts.append("\n=== RESPONSE BODY (SAMPLE) ===")
        
        # Look for error messages, debug info, etc.
        error_indicators = [
            'error', 'exception', 'stack trace', 'warning', 'notice',
            'debug', 'phpinfo', 'var_dump', 'print_r', 'console.log'
        ]
        
        body_lines = entry.response_body.split('\n')
        relevant_lines = []
        
        for line in body_lines:
            line_lower = line.lower()
            if any(indicator in line_lower for indicator in error_indicators):
                relevant_lines.append(line.strip())
                if len(relevant_lines) >= 10:  # Limit relevant lines
                    break
        
        if relevant_lines:
            content_parts.extend(relevant_lines)
        else:
            # Add first part of body if no specific indicators found
            content_parts.append(entry.response_body[:1500])
        
        full_content = "\n".join(content_parts)
        
        if len(full_content) > max_content_length:
            full_content = full_content[:max_content_length] + "... [TRUNCATED]"
        
        return full_content
    
    def _create_vulnerability_prompt(self, content: str, url: str) -> str:
        """Create prompt for AI vulnerability analysis"""
        return f"""
Analyze the following web application response for potential security vulnerabilities. Look for:

1. Information disclosure (error messages, debug info, version info)
2. Missing security headers
3. Weak authentication/session management
4. Input validation issues
5. Configuration problems
6. Any other security concerns

URL: {url}

Response Content:
{content}

Please respond with a JSON array of vulnerability objects. Each object should have:
- "type": vulnerability category
- "severity": "Critical", "High", "Medium", or "Low"
- "confidence": float between 0.0 and 1.0
- "description": detailed explanation
- "evidence": specific evidence from the content

Example format:
[
  {{
    "type": "Information Disclosure",
    "severity": "Medium",
    "confidence": 0.8,
    "description": "Server version information disclosed in headers",
    "evidence": "Server: Apache/2.4.41"
  }}
]

Only include vulnerabilities you're confident about. Return empty array [] if no clear vulnerabilities are found.
"""
    
    async def _get_ai_response(self, prompt: str) -> str:
        """Get response from AI provider"""
        if self.config.llm_provider == "openai":
            response = await self.client.chat.completions.create(
                model=self.config.llm_model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1000,
                temperature=0.1
            )
            return response.choices[0].message.content
        
        elif self.config.llm_provider == "anthropic":
            response = await self.client.messages.create(
                model=self.config.llm_model,
                max_tokens=1000,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.content[0].text
        
        return ""
    
    def _parse_vulnerabilities(self, response: str, url: str) -> List[Dict]:
        """Parse vulnerabilities from AI response"""
        try:
            import json
            vulnerabilities = json.loads(response.strip())
            
            if isinstance(vulnerabilities, list):
                # Add metadata to each vulnerability
                for vuln in vulnerabilities:
                    if isinstance(vuln, dict):
                        vuln['location'] = {'url': url, 'type': 'ai_detected'}
                        vuln['timestamp'] = datetime.now().isoformat()
                        vuln['source'] = 'AI Analysis'
                
                return vulnerabilities
        
        except json.JSONDecodeError:
            # Log the failed parsing but don't crash
            self.logger.warning(f"Failed to parse AI vulnerability response: {response[:200]}")
        
        return []
